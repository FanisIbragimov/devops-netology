**Основная часть**     
Новые проекты пошли стабильным потоком. Каждый проект требует себе несколько кластеров: под тесты и продуктив.    
Делать все руками — не вариант, поэтому стоит автоматизировать подготовку новых кластеров.      
    
**Задание №1 Подготовить инвентарь kubespray**     
   
Новые тестовые кластеры требуют типичных простых настроек. Нужно подготовить инвентарь и проверить его работу. Требования к инвентарю:    
подготовка работы кластера из 2 нод: 1 мастер и 1 рабочая нода;      
в качестве CRI — containerd;     
запуск etcd производить на мастере.    
 
    
**Решение**   
Создали два хоста с помощью скрипта    
```
#!/bin/bash

set -e

function create_vm {
  local NAME=$1

  YC=$(cat <<END
    yc compute instance create \
      --name $NAME \
      --hostname $NAME \
      --zone ru-central1-a \
      --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 \
      --memory 2 \
      --cores 2 \
      --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts,type=network-ssd,size=20 \
      --ssh-key /home/ubuntu/.ssh/id_rsa.pub
END
)
#  echo "$YC"
  eval "$YC"
}

create_vm "cp1"
create_vm "node1"
#create_vm "node2"
```
    
Далее изменили файл hosts.yaml с указанием внешних ip адресов и имени пользователя      
```
all:
  hosts:
    cp1:
      ansible_host: 51.250.88.98
      ansible_user: yc-user
    node1:
      ansible_host: 51.250.65.78
      ansible_user: yc-user
  children:
    kube_control_plane:
      hosts:
        cp1:
    kube_node:
      hosts:
        cp1:
        node1:
    etcd:
      hosts:
        cp1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}

```
Для просмотра kubectl get pods на локальном компьютере внесли изменения в строку supplementary_addresses_in_ssl_keys: [51.250.88.98] в файле k8-cluster.yml
```
---
supplementary_addresses_in_ssl_keys: [51.250.88.98]
---
```
Запустили установку кластера:
```
ansible-playbook -i inventory/mycluster/hosts.yaml  --become --become-user=root cluster.yml
```
